{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d924582",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "from sagemaker.serverless import ServerlessInferenceConfig\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "# --- Setup ---\n",
    "sagemaker_session = sagemaker.Session()\n",
    "# Use the default S3 bucket for uploading model artifacts\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Define the local folder containing inference.py and requirements.txt\n",
    "source_dir = 'code' \n",
    "\n",
    "# --- 1. Model Packaging (Requires model weights to be downloaded first) ---\n",
    "# NOTE: This step assumes you have already downloaded the Stable Diffusion 1.5,\n",
    "# ControlNet, and IP-Adapter weights and placed them alongside the 'code' folder \n",
    "# in the structure required by inference.py.\n",
    "\n",
    "# Create the model.tar.gz artifact\n",
    "!tar -czvf model.tar.gz ./code ./models/stable-diffusion-v1-5 ./models/controlnet ./models/ip_adapter_models\n",
    "\n",
    "# Upload the artifact to S3\n",
    "model_data_uri = sagemaker_session.upload_data(\n",
    "    path='model.tar.gz', \n",
    "    key_prefix='stable-diffusion-inference/model_artifacts'\n",
    ")\n",
    "print(f\"Model artifact uploaded to: {model_data_uri}\")\n",
    "\n",
    "# --- 2. Define Container Image ---\n",
    "# Retrieve the necessary HuggingFace PyTorch CPU Inference Image (Serverless doesn't use GPU)\n",
    "# IMPORTANT: Use the correct versions compatible with your downloaded models.\n",
    "# The memory size limitation requires a CPU image.\n",
    "region = boto3.Session().region_name\n",
    "inference_image_uri = retrieve(\n",
    "    framework='huggingface',\n",
    "    region=region,\n",
    "    version='1.13', # PyTorch version (needs to be compatible with Diffusers)\n",
    "    py_version='py39', \n",
    "    instance_type='ml.c5.xlarge', # Specify a compatible instance type for URI retrieval\n",
    "    accelerator_type=None,\n",
    "    image_scope='inference'\n",
    ")\n",
    "\n",
    "# --- 3. Create Serverless Configuration (Addressing Limitations) ---\n",
    "# Set the maximum memory size and concurrency. \n",
    "# 6144 MB is required to load the full SD + ControlNet + IP-Adapter pipeline.\n",
    "serverless_config = ServerlessInferenceConfig(\n",
    "    memory_size_in_mb=6144,  # Max memory to prevent OOM errors (16GB RAM hit limitation)\n",
    "    max_concurrency=2,       # Low concurrency for initial cost control\n",
    "    # min_concurrency=1      # For V2, uncomment this to reduce cold start latency\n",
    ")\n",
    "\n",
    "# --- 4. Create HuggingFace Model Object ---\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=model_data_uri,\n",
    "    role=role,\n",
    "    image_uri=inference_image_uri,\n",
    "    # The entry_point points to the custom inference script inside the S3 artifact\n",
    "    entry_point='inference.py',\n",
    "    source_dir='./code', # Tells SageMaker where to find the inference script inside the tarball\n",
    "    env={\n",
    "        'SAGEMAKER_PROGRAM': 'inference.py', \n",
    "        'MMS_MAX_WORKERS': '1' # Control the number of workers to manage memory\n",
    "    }\n",
    ")\n",
    "\n",
    "# --- 5. Deploy Endpoint ---\n",
    "# Deploy the model using the serverless configuration\n",
    "predictor = huggingface_model.deploy(\n",
    "    serverless_inference_config=serverless_config,\n",
    "    endpoint_name=f'sd-serverless-endpoint-{sagemaker.utils.unique_name_from_base(\"sd\")}',\n",
    "    wait=True \n",
    ")\n",
    "\n",
    "print(f\"Endpoint Name: {predictor.endpoint_name}\")\n",
    "# Save the endpoint name to an environment variable for the FastAPI Lambda function\n",
    "# os.environ['SAGEMAKER_ENDPOINT_NAME'] = predictor.endpoint_name\n",
    "\n",
    "# --- 6. Cleanup (Crucial for cost control) ---\n",
    "# predictor.delete_endpoint() \n",
    "# predictor.delete_model()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
